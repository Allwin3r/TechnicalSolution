В работе по прогнозированию стоимости медицинской страховки были использованы современные инструменты анализа данных и машинного обучения из экосистемы Python. Для обработки данных применялись библиотеки pandas и numpy, обеспечивающие эффективную загрузку, фильтрацию и подготовку признаков. Основной поток предобработки строился с помощью scikit-learn, где все этапы (масштабирование, кодирование категорий, генерация новых признаков) были интегрированы через pipeline и column transformer.

Для повышения качества моделирования особое внимание уделялось feature engineering: кроме базовых признаков, были добавлены интеракции, такие как произведения возраста и BMI, а также курения и BMI. Категориальные признаки (пол, курение) переводились в бинарный формат, регион — в one-hot encoding, что позволяло любым моделям работать с ними напрямую.

Целевая переменная — итоговая стоимость страховки — логарифмировалась (np.log1p), чтобы уменьшить влияние редких, но крупных выплат (выбросов), что особенно важно для устойчивости моделей на реальных страховых данных.

Среди моделей тестировались как классические, так и современные методы. В качестве базовой использовалась линейная регрессия (LinearRegression из sklearn): она проста для интерпретации, но показала ограниченную точность на этих данных (R² ~ 0.54), так как не способна улавливать сложные нелинейные зависимости между признаками. Для проверки эффективности ансамблей были реализованы модели XGBoost (библиотека xgboost) с разными типами бустеров: стандартный градиентный бустинг (gbtree), бустер с dropout деревьев (dart) и даже линейный бустер (gblinear), который по сути является аналогом линейной регрессии с регуляризацией.

Также сравнивалась производительность таких библиотек, как LightGBM и CatBoost, что позволило убедиться в конкурентоспособности XGBoost на данном датасете. Параметры моделей подбирались через GridSearchCV с кросс-валидацией, что гарантировало честную оценку и предотвращение переобучения. Для визуализации качества работы моделей использовались matplotlib и seaborn: были построены scatter-графики “истинное vs предсказанное”, распределения ошибок и кривые обучения.

В финальной версии пайплайна наилучшие результаты показал XGBoost с бустером “dart” и глубиной дерева 4, что обеспечило R² ≈ 0.88 и среднюю абсолютную ошибку ниже 2000 у.е. Такой результат был недостижим ни одной линейной моделью, ни простыми деревьями решений. При этом для задач анализа и контроля возможных рисков были построены и ROC-кривые (с пороговой бинаризацией по стоимости страховки), позволяющие анализировать не только точность регрессии, но и способность системы выделять критически дорогие страховые случаи.
